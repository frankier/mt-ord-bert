link_rt_exp_results.goodclass.nodiscrim contains a run with:
 * A good classifier run
 * Everything else with single discrimination (bad)
 * SLURM logs in link_rt_exp_results.goodclass.nodiscrim/logs

2022-11-29 9:30
Starting a run which
 * Logs to W&B
 * Misses out class since this was fairly successful already last time and has no tweaks
 * Everything else gets a run with per_task and multi discrimination
   * Previously everything was run with single discrimination which is probably pretty much useless/wrong

16:04 start again: only add_discrims.22112910/jsons/rt_fwd_cumulative_per_task.json
17:38 again
21:08 again
23:43 again
2022-11-30 9:00 again: seems to be training now

2022-11-30 9:50:
It looks like the boundaries have not been being learnt because of a method
which doesn't propagate gradients which probably explains the lousy performance

10:00 Starting training of:
rungroup__fix_thresholds.22113010/jsons/rt_fwd_acat_*.json
rungroup__fix_thresholds.22113010/jsons/rt_fwd_cumulative_*.json

11:30 Got allocation

Later:
It looks like what's happening is that the boundaries are still not being
learnt well. It may be that each group gets too little "air time".

2022-12-01 12:00
Starting a run with just the biggest group (ds:rt1). Calling this dataset rt_one. This should help figure out for sure whether the problem is the multi-task setting (I think it is).

Beforehand I also reparameterised the affine layer to be discrimination * input
+ offsets rather than discrimination * (input + offsets) meaning the true
offsets are offsets / discrimination. Hopefully this is easier to learn (now it
is the same as a standard linear).

16:50
Starting again after hopefully fixing a couple of bugs

2022-12-02 7:30
Some more bugs in the single group version found. Trying to fix them.

9:00: Schedule another run on the rt1 ds with fwd_acat, fwd_cumulative and class.

12:45: Looking at the results, fwd_acat seems broken. But fwd_cumulative and class
do okay. I'm not clear if the schedule gives them enough time to learn
properly e.g. would they benefit from more epochs. Quite possibly since there is less data.

2022-12-04
16:40
Trying rt40k with a higher learning rate of 2e-5 rather than 1e-5. Also using
0.1 warmup ratio. There have been small improvements since the last time it was run and now everything is on w&b and we have a multiscale mae. Trying the following models: fwd_cumulative_per_task fwd_cumulative_multi & class.

17:40
It seems that everything currently still runs per_task, ignoring this part of the configuration. It has also been that the rt1 models are forced to run as discrimination_mode=none but they can actually have multi
