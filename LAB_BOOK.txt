link_rt_exp_results.goodclass.nodiscrim contains a run with:
 * A good classifier run
 * Everything else with single discrimination (bad)
 * SLURM logs in link_rt_exp_results.goodclass.nodiscrim/logs

2022-11-29 9:30
Starting a run which
 * Logs to W&B
 * Misses out class since this was fairly successful already last time and has no tweaks
 * Everything else gets a run with per_task and multi discrimination
   * Previously everything was run with single discrimination which is probably pretty much useless/wrong

16:04 start again: only add_discrims.22112910/jsons/rt_fwd_cumulative_per_task.json
17:38 again
21:08 again
23:43 again
2022-11-30 9:00 again: seems to be training now

2022-11-30 9:50:
It looks like the boundaries have not been being learnt because of a method
which doesn't propagate gradients which probably explains the lousy performance

10:00 Starting training of:
rungroup__fix_thresholds.22113010/jsons/rt_fwd_acat_*.json
rungroup__fix_thresholds.22113010/jsons/rt_fwd_cumulative_*.json

11:30 Got allocation

Later:
It looks like what's happening is that the boundaries are still not being
learnt well. It may be that each group gets too little "air time".

2022-12-01 12:00
Starting a run with just the biggest group (ds:rt1). Calling this dataset rt_one. This should help figure out for sure whether the problem is the multi-task setting (I think it is).

Beforehand I also reparameterised the affine layer to be discrimination * input
+ offsets rather than discrimination * (input + offsets) meaning the true
offsets are offsets / discrimination. Hopefully this is easier to learn (now it
is the same as a standard linear).

16:50
Starting again after hopefully fixing a couple of bugs

2022-12-02 7:30
Some more bugs in the single group version found. Trying to fix them.

9:00: Schedule another run on the rt1 ds with fwd_acat, fwd_cumulative and class.

12:45: Looking at the results, fwd_acat seems broken. But fwd_cumulative and class
do okay. I'm not clear if the schedule gives them enough time to learn
properly e.g. would they benefit from more epochs. Quite possibly since there is less data.

2022-12-04
16:40
Trying rt40k with a higher learning rate of 2e-5 rather than 1e-5. Also using
0.1 warmup ratio. There have been small improvements since the last time it was run and now everything is on w&b and we have a multiscale mae. Trying the following models: fwd_cumulative_per_task fwd_cumulative_multi & class.

17:40
It seems that everything currently still runs per_task, ignoring this part of the configuration. It has also been that the rt1 models are forced to run as discrimination_mode=none but they can actually have multi

2022-12-04
8:40
Okay I think the discrimination_mode problem has now finally been fixed. Running rt_fwd_cumulative_multi.slurm and rt_one_fwd_cumulative_multi.slurm

2022-12-06
14:45

I think the problem is the optimization of the cutoffs is counter to the BERT
optimization. The analogy would be the way BatchNorm will not use running mean
and variance statistics. Here the cutoff optimization is essentially trying to
learn summary statistics of a hidden scale which is being learnt at the same
time. One possibility then would be to re-estimate the cutoffs using only the
mini-batch data. Another possibility would be to initialize with either
MT-softmax or MS-linear. In the former case we can think about re-parameterising
to be like the adjacent category model, and in the latter like the cumulative
probability model. One interesting possibility is having softmax regression
learnt with a single hidden linear and then reparameterising this, which would
give an adjacent category model directly, without a need for further training.

2022-12-07
20:05

I have made quite a few runs on a dataset made to show the benefit of the
approach. The ones that work now are a new MS-linear model (aka regress) and
MT-softmax (aka class). The dataset is comprised of 5 critics who have
irregular rating patterns.

22:05 Re-running jobs/rt_irr5_fwd_cumulative_per_task. It should be fixed now.

2022-12-08

8:30 Looks like jobs/rt_irr5_fwd_cumulative_per_task is in the same state for
this dataset. I'm not sure exactly why the learning hasn't happened.

9:00 Starting latent_softmax, which is a softmax regression model with a shared
latent scale.

9:50 latent_softmax works less well than the explicitly ordinal
parameterisations. This really confirms that the problem is the optimization of
the cutoffs / task/scale specific parameters.

10:20

Okay. The agenda is: first get it working for a single group, then irr5. Here
are some possible approaches:

 * Train with linear or softmax first, then copy weights over to ordinal and
   carry on training
 * Better initialisation.
 * Better diagnostics. How far exactly are the final cutoffs from their initialisation?

2020-12-12

10:50

I have made some runs with careful quantile initialisation. They show some
promise but do not yet beat the MS-linear baseline. There have also been some
problems getting the initailisation correct, including making sure the hidden
linear is actually standarised.

I have also switched to a slower 20 epoch schedule following

Mosbach, M., Andriushchenko, M., & Klakow, D. (2020). On the Stability of
Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. ArXiv,
abs/2006.04884.

Generally the accuracy curves on the development set have been really noisy. I
think this means the learning rate might be a bit too high. However, the
quantile initialisation has a parameter about how much probability each class
should get. Initially this was probably a bit too high, 0.8. I have now set
this to 0.5. Part of the noisiness may come from bad learning from the bad
initalisation resulting in very high loss rates at the beginning. 

All in all this has not really worked. This seems like a bit of a dead end.
Without either fitting some kind of fittable model first, and then fitting an
ordinal model so as to get better initialisation, or turning to meta-learning
its not clear what can be done better. Perhaps sufficient parameter turning
would lead to a more stable learning process, but it's certainly not clear at
the moment.

2020-12-13

9:00

I think the losses for misclassification might be a bit too big. I am switching
to a non-statistically modelled threshold model with hinge losses. See:

https://home.ttic.edu/~nati/Publications/RennieSrebroIJCAI05.pdf

This is a bit like a cross between the regression model and the ordinal
regression model. Hopefully this will at least provide a starting point for
further investigation.

2020-12-14

13:30

This threshold does not seem to train at all. There needs to be some constraint
to keep the thresholds from shrinking on top of one another. I tried adding a
BatchNorm just before to force everything into a reasonable range, but this
didn't work. I will put this model on hold for now, but might return to it
later.

I then took a look at whether its possible to convert a softmax model into an
adjacent categories model. It seems that we could just get the weight for e.g.
P(X=2|X\in1,2) by taking \Beta_2-\Beta_1 and so on. We could then shrink these
weights to be closer to one another or similar. In fact, if the softmax model
is already learning a scale then these weights could be collerated as we
compare categories up the scale. However, this not seem to be the case. I'm not
entirely sure whether I have made an error here, but it doesn't mean that it
might not be possible to start off with a softmax and slowly penalise it
towards an adjacent category model.

I have implemented a version of the threshold model with fixed thresholds based
on the label distribution the training dataset, but that allows learnable
per-task linears as with MS-linear. I will run this one next.

15:45

That did not work at all. No idea why. The initial loss was kind of low so I
tried increasing the learning rate to 1e-4, but it didn't work. Both threshold
models could do with revisiting.

2020-12-15

17:00

I have now tried fitting thresholds (using the VGAM R package) based on
MS-linear and then attaching this to a fresh fwd_cumulative_multi body. It did
not really work. I still need to a take a look at whether the cutoffs look
sane. The next thing to try is to also transfer body weights.

It's worth noting that the 3 epoch run of fwd_cumulative_multi after adding in
quantile initialisation worked reasonably well, even if it did not quite manage
to beat MS-linear. This one with the cutoffs does not work at all

I've been thinking a bit about what the objectives here. Two thoughts about the
objectives which may help to tackle them separately:

 * With the ordinal models, the aim is to simultaneously estimate the
   distribution (mean, std dev) of the labels with respect to the hidden scale
   while also improving the quality of the hidden scale. These objectives may
   work slightly against each other. The MT-softmax regression trains very
   smoothly, and MS-linear is almost as smooth (try smooth L1 to improve
   regression?).
 * The multi-scale set up is inherently upsetting to trying to use MS-linear
   since there is not enough ``squishability'' of the common scale. MS-linear
   correctly seperates out the means of the labels, but each scale has
   different spacing of these means.

2020-12-17

13:00

There is a paper about OLL -- alternatives to softmax loss that take into
account category distance. It would be nice to implement these and try them
with this data. I'm interested in whether this would push it towards adjacent
categories on a single dimension. It is possible that dropout in the final
layer adds noise which stops this happening.

It is quite well known how to fine-tune BERT with contrastive loss/metric
learning. It's worth seeing if that would help here since then we would
probably not have the unstablity which appears to come from trying to estimate
bondaries at the same time as pushing points towards the correct clusters.

2020-12-27

I have tried implementing an ad-hoc metric learning method I just came up with,
and it worked okay. I have also implemented fitting of the models during
evaluation.

I'm not really sure why the threshold model performed so much worse than just
about every other model, so I will try and get it working again next.

The only model which has very smooth learning rate is the classification one.
Every other model has some noise during training. I think this may be a
combination of the fact that the data is quite noisy and the other loss
functions are quite sensitive to outliers. L1 or some smooth L1 regression
could be investigated for regression. This is a good argument for the threshold
model since it should be easy-ish to plug in some more robust loss there.
